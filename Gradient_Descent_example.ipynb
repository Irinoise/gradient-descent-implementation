{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Irinoise/gradient-descent-implementation/blob/main/Gradient_Descent_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwCOPGI_DYdY"
      },
      "source": [
        "# Реализация градиентного спуска"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHVxhkVUDYdv"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class LinearReg(BaseEstimator):\n",
        "    def __init__(self, gd_type='full',\n",
        "                 tolerance=1e-6, max_iter=10000, eta=1e-2):\n",
        "        \"\"\"\n",
        "        gd_type: 'full' or 'stochastic'\n",
        "        tolerance: for stopping gradient descent\n",
        "        max_iter: maximum number of steps in gradient descent\n",
        "        w0: np.array of shape (d) - init weights\n",
        "        eta: learning rate\n",
        "        \"\"\"\n",
        "        self.gd_type = gd_type\n",
        "        self.tolerance = tolerance\n",
        "        self.max_iter = max_iter\n",
        "        self.w = None\n",
        "        self.eta = eta\n",
        "        self.loss_history = None\n",
        "\n",
        "    def fit(self, features, target):\n",
        "        \"\"\"\n",
        "        X: np.array of shape (ell, d)\n",
        "        y: np.array of shape (ell)\n",
        "        ---\n",
        "        output: self\n",
        "        \"\"\"\n",
        "        self.loss_history = []\n",
        "        w_new = []\n",
        "        i = 0\n",
        "\n",
        "        if self.gd_type == 'stochastic':\n",
        "            X_len = features.shape[0]- 1\n",
        "            index = np.random.randint(X_len)\n",
        "            X = features[index: index+1]\n",
        "            y = target[index: index+1]\n",
        "        elif self.gd_type == 'full':\n",
        "            X = features\n",
        "            y = target\n",
        "\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "        self.loss_history.append(self.calc_loss(X, y))\n",
        "\n",
        "        while True:\n",
        "            gradient = self.calc_gradient(X, y)\n",
        "            w_new = self.w - (self.eta * gradient)\n",
        "            if np.linalg.norm(w_new - self.w) < self.tolerance:\n",
        "                print('weights distance < tolerance')\n",
        "                break\n",
        "\n",
        "            self.w = w_new\n",
        "            loss = self.calc_loss(X, y)\n",
        "            self.loss_history.append(loss)\n",
        "\n",
        "            print(\n",
        "                f\"iter = {i};\"\n",
        "                f\"loss = {loss}\"\n",
        "                )\n",
        "\n",
        "            i += 1\n",
        "            if self.gd_type == 'stochastic':\n",
        "                index = np.random.randint(X_len)\n",
        "                X = features[index: index+1]\n",
        "                y = target[index: index+1]\n",
        "\n",
        "            if i > self.max_iter:\n",
        "                print('max iter reached')\n",
        "                break\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.w is None:\n",
        "            raise Exception('Not trained yet')\n",
        "\n",
        "        else:\n",
        "            y_pred = X.dot(self.w)\n",
        "            return y_pred\n",
        "\n",
        "    def calc_gradient(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array of shape (ell, d) (ell can be equal to 1 if stochastic)\n",
        "        y: np.array of shape (ell)\n",
        "        ---\n",
        "        output: np.array of shape (d)\n",
        "        \"\"\"\n",
        "        # grad = 2Xt(Xw-y)\n",
        "        n = X.shape[0]\n",
        "        e = X.dot(self.w) - y\n",
        "        grad = np.dot(np.transpose(X), e)\n",
        "        return 2/n * grad\n",
        "\n",
        "    def calc_loss(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array of shape (ell, d)\n",
        "        y: np.array of shape (ell)\n",
        "        ---\n",
        "        output: float\n",
        "        \"\"\"\n",
        "        y_predicted = self.predict(X)\n",
        "        mse = np.mean((y - y_predicted)**2)\n",
        "        return mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iecpk27oDYdx"
      },
      "source": [
        "* Загрузим данные;\n",
        "* Разобьем выборку на обучающую и тестовую в отношении 7:3 с random_seed=0;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR2JPB85DYdy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "9ea23c7d-4821-4313-f19b-b95da29f7fb9"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "!wget -O 'dataset_hw2.csv' -q 'https://www.dropbox.com/s/jh3n0d05vi7azzk/export_dataframe.csv?dl=0'\n",
        "\n",
        "data = pd.read_csv('dataset_hw2.csv')\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>month</th>\n",
              "      <th>day_of_year</th>\n",
              "      <th>hour</th>\n",
              "      <th>is_abnormal_january</th>\n",
              "      <th>is_abnormal_may</th>\n",
              "      <th>log_haversine</th>\n",
              "      <th>pickup_la_guardia</th>\n",
              "      <th>dropoff_la_guardia</th>\n",
              "      <th>pickup_john_f_kennedy</th>\n",
              "      <th>dropoff_john_f_kennedy</th>\n",
              "      <th>is_traffic_jam</th>\n",
              "      <th>is_free_road</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>log_trip_duration</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>87</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.524717</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7.134891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>78</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.753154</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.878326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>106</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.352776</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>7.642524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>84</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.057223</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.888572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>119</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.521534</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.177944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455095</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.014222</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>7.152269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455096</th>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>154</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.940251</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.940222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455097</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>123</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.879760</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>7.003974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455098</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>97</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.621334</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>7.619724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455099</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>77</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.929858</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.530878</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1455100 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         day_of_week  month  ...  passenger_count  log_trip_duration\n",
              "0                  6      3  ...                3           7.134891\n",
              "1                  4      3  ...                1           6.878326\n",
              "2                  4      4  ...                2           7.642524\n",
              "3                  3      3  ...                1           6.888572\n",
              "4                  3      4  ...                1           6.177944\n",
              "...              ...    ...  ...              ...                ...\n",
              "1455095            5      1  ...                4           7.152269\n",
              "1455096            3      6  ...                1           6.940222\n",
              "1455097            0      5  ...                1           7.003974\n",
              "1455098            2      4  ...                1           7.619724\n",
              "1455099            3      3  ...                1           6.530878\n",
              "\n",
              "[1455100 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-w4E68-DYd1"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = data['log_trip_duration']\n",
        "X = data.drop(columns=['log_trip_duration'])\n",
        "\n",
        "categorical = ['day_of_week', 'month', 'day_of_year', 'hour', 'passenger_count', 'is_abnormal_january', 'is_abnormal_may', 'is_traffic_jam', 'is_free_road', 'pickup_la_guardia', 'dropoff_la_guardia', 'pickup_john_f_kennedy', 'dropoff_john_f_kennedy']\n",
        "numeric = ['log_haversine']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "one_hot_enc = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "scaled_columns = scaler.fit_transform(X[numeric])\n",
        "encoded_columns = one_hot_enc.fit_transform(X[categorical]).toarray()\n",
        "\n",
        "X_fin = np.concatenate([scaled_columns, encoded_columns], axis=1)\n",
        "\n",
        "# Добавим в X столбец с единицами, чтобы упростить вычисления в матричном виде\n",
        "X_fin = np.column_stack([np.ones([X_fin.shape[0], 1], dtype=np.int32), X_fin])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9KeswBNDYd2"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_fin, y, test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv-e44pYodX6"
      },
      "source": [
        "**Комментарий:**  \n",
        "\n",
        "Значения метрик MSE для стохастического градиентного спуска в среднем несколько хуже, чем для полного градиентного спуска.\n",
        "\n",
        "**Полный градиентный спуск** с параметрами `max_iter` = 10000 и `eta` = 1e-2 показал следующие результаты: MSE = 0.2249\n",
        "\n",
        "**Стохастический градиентный спуск** с теми же параметрами показал следующие результаты на 10 запусках:\n",
        "\n",
        "1) MSE = 0.2361\n",
        "\n",
        "2) MSE = 0.2572\n",
        "\n",
        "3) MSE = 0.2619\n",
        "\n",
        "4) MSE = 0.2401\n",
        "\n",
        "5) MSE = 0.2442\n",
        "\n",
        "6) MSE = 0.2636\n",
        "\n",
        "7) MSE = 0.2795\n",
        "\n",
        "8) MSE = 0.2670\n",
        "\n",
        "9) MSE = 0.2488\n",
        "\n",
        "10) MSE = 0.2612\n",
        "\n",
        "Влияние параметров `max_iter` и `eta` согласуется с ожиданиями. Так, количество итераций влияет на число шагов, которые делаются по антиградиенту. Таким образом, если не выполняется условие останова по разнице весов, то большее значение `max_iter` приведет в результате к меньшему значению функции потерь при одинаковых `eta`. Learning rate (`eta`) в свою очередь влияет на величину спуска на каждой итерации. Так, большее значение `eta` приводит к большему спуску на шаге –> значение функции потерь будет уменьшаться на бОльшую величину (при большом значении `eta` есть шанс перескочить минимум функции).\n",
        "\n",
        "Для примера градиентный спуск запускался на двух значениях `eta = 1e-2` и `eta = 1e-1`.\n",
        "\n",
        "Получены такие значения MSE на соответствующих шагах (с точностью до 4-х знаков после запятой):\n",
        "\n",
        "При `eta = 1e-2`:\n",
        "\n",
        "`iter` = 100: **0.3031**, `iter` = 200: **0.2677**, `iter` = 500: **0.2441**, `iter` = 1000: **0.2358**, `iter` = 5000: **0.2662**, `iter` = 10000: **0.2245**\n",
        "\n",
        "При `eta = 1e-1`:\n",
        "\n",
        "`iter` = 100: **0.2357**, `iter` = 200: **0.2304**, `iter` = 500: **0.2262**, `iter` = 1000: **0.2245**, `iter` = 5000: **0.2237**, `iter` = 10000: **0.2237** (отличие от значения MSE на шаге 5000 – в 5-м знаке после запятой).\n",
        "\n",
        "Видим, что значение, которое получено на 1000 шаге при `eta = 1e-1`, при `eta = 1e-2` достигается только на шаге порядка 10000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "071VIUmkDYd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "41de38f9-5e27-422e-a42b-142da0e1bbed"
      },
      "source": [
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # подставляя различные параметры, можно наблюдать процесс (значение функции потерь на каждой итерации) и результаты (MSE и R2) градиентного спуска\n",
        "    # например, comp = LinearReg(gd_type='stochastic') – запускает SGD с max_iter=10000 и eta=1e-2\n",
        "    # или comp = LinearReg(eta=1e-1) – запускает полный градиентный спуск с max_iter=10000 и eta=1e-1\n",
        "    comp = LinearReg()\n",
        "    comp.fit(X_train, y_train)\n",
        "    y_pred = comp.predict(X_test)\n",
        "\n",
        "    # r2\n",
        "    print(\"R2 = %.4f\" % r2_score(y_test, y_pred))\n",
        "\n",
        "    # MSE\n",
        "    print(\"MSE = %.4f\" % mean_squared_error(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter = 0;loss = 28.105897345831306\n",
            "iter = 1;loss = 18.661034889133717\n",
            "iter = 2;loss = 12.471849734488464\n",
            "iter = 3;loss = 8.414317944443786\n",
            "iter = 4;loss = 5.752553275746294\n",
            "iter = 5;loss = 4.004776318366845\n",
            "iter = 6;loss = 2.8555669863340336\n",
            "iter = 7;loss = 2.0984178529535567\n",
            "iter = 8;loss = 1.5981248820132643\n",
            "iter = 9;loss = 1.2661653312288341\n",
            "iter = 10;loss = 1.0445781146770963\n",
            "iter = 11;loss = 0.8954105664067571\n",
            "iter = 12;loss = 0.7938094231335378\n",
            "iter = 13;loss = 0.7234976093778217\n",
            "iter = 14;loss = 0.673812963212022\n",
            "iter = 15;loss = 0.6377695356881438\n",
            "iter = 16;loss = 0.6107883504970638\n",
            "iter = 17;loss = 0.5898664470824164\n",
            "iter = 18;loss = 0.5730328600861149\n",
            "iter = 19;loss = 0.5589924509833764\n",
            "iter = 20;loss = 0.5468927233615534\n",
            "iter = 21;loss = 0.5361711536064204\n",
            "iter = 22;loss = 0.5264552338240442\n",
            "iter = 23;loss = 0.5174970247707961\n",
            "iter = 24;loss = 0.5091303021211533\n",
            "iter = 25;loss = 0.5012424944415054\n",
            "iter = 26;loss = 0.49375630527727743\n",
            "iter = 27;loss = 0.486617675496868\n",
            "iter = 28;loss = 0.47978789672262534\n",
            "iter = 29;loss = 0.4732384426310524\n",
            "iter = 30;loss = 0.46694757981130863\n",
            "iter = 31;loss = 0.4608981438795247\n",
            "iter = 32;loss = 0.455076078667616\n",
            "iter = 33;loss = 0.44946947517772884\n",
            "iter = 34;loss = 0.44406793791162713\n",
            "iter = 35;loss = 0.4388621657060146\n",
            "iter = 36;loss = 0.4338436731733986\n",
            "iter = 37;loss = 0.4290046043615239\n",
            "iter = 38;loss = 0.4243376069442643\n",
            "iter = 39;loss = 0.41983574619546454\n",
            "iter = 40;loss = 0.41549244515362294\n",
            "iter = 41;loss = 0.41130144207468444\n",
            "iter = 42;loss = 0.40725675933779343\n",
            "iter = 43;loss = 0.4033526799788114\n",
            "iter = 44;loss = 0.39958372934236985\n",
            "iter = 45;loss = 0.39594466020434893\n",
            "iter = 46;loss = 0.3924304402809215\n",
            "iter = 47;loss = 0.3890362414105429\n",
            "iter = 48;loss = 0.3857574299369151\n",
            "iter = 49;loss = 0.3825895579790805\n",
            "iter = 50;loss = 0.37952835538087226\n",
            "iter = 51;loss = 0.37656972219746826\n",
            "iter = 52;loss = 0.3737097216243859\n",
            "iter = 53;loss = 0.3709445733014041\n",
            "iter = 54;loss = 0.368270646945531\n",
            "iter = 55;loss = 0.36568445627830676\n",
            "iter = 56;loss = 0.3631826532220624\n",
            "iter = 57;loss = 0.36076202234571925\n",
            "iter = 58;loss = 0.35841947554389875\n",
            "iter = 59;loss = 0.356152046936561\n",
            "iter = 60;loss = 0.3539568879773188\n",
            "iter = 61;loss = 0.3518312627610888\n",
            "iter = 62;loss = 0.34977254352124526\n",
            "iter = 63;loss = 0.3477782063087112\n",
            "iter = 64;loss = 0.34584582684465287\n",
            "iter = 65;loss = 0.34397307654014186\n",
            "iter = 66;loss = 0.3421577186749734\n",
            "iter = 67;loss = 0.3403976047302569\n",
            "iter = 68;loss = 0.3386906708676423\n",
            "iter = 69;loss = 0.337034934549708\n",
            "iter = 70;loss = 0.33542849129560287\n",
            "iter = 71;loss = 0.33386951156681394\n",
            "iter = 72;loss = 0.3323562377772213\n",
            "iter = 73;loss = 0.33088698142330336\n",
            "iter = 74;loss = 0.3294601203288556\n",
            "iter = 75;loss = 0.3280740960001595\n",
            "iter = 76;loss = 0.32672741108695\n",
            "iter = 77;loss = 0.3254186269450422\n",
            "iter = 78;loss = 0.3241463612964209\n",
            "iter = 79;loss = 0.3229092859830603\n",
            "iter = 80;loss = 0.32170612481042904\n",
            "iter = 81;loss = 0.3205356514776894\n",
            "iter = 82;loss = 0.31939668759017126\n",
            "iter = 83;loss = 0.3182881007517785\n",
            "iter = 84;loss = 0.31720880273352103\n",
            "iter = 85;loss = 0.3161577477154775\n",
            "iter = 86;loss = 0.3151339305988173\n",
            "iter = 87;loss = 0.3141363853856042\n",
            "iter = 88;loss = 0.3131641836232235\n",
            "iter = 89;loss = 0.3122164329113021\n",
            "iter = 90;loss = 0.31129227546784954\n",
            "iter = 91;loss = 0.3103908867532971\n",
            "iter = 92;loss = 0.3095114741492123\n",
            "iter = 93;loss = 0.3086532756899225\n",
            "iter = 94;loss = 0.3078155588449386\n",
            "iter = 95;loss = 0.30699761934999226\n",
            "iter = 96;loss = 0.30619878008468726\n",
            "iter = 97;loss = 0.30541838999543625\n",
            "iter = 98;loss = 0.30465582306084693\n",
            "iter = 99;loss = 0.30391047729900356\n",
            "iter = 100;loss = 0.30318177381415656\n",
            "iter = 101;loss = 0.30246915588143924\n",
            "iter = 102;loss = 0.30177208806851374\n",
            "iter = 103;loss = 0.30109005539192124\n",
            "iter = 104;loss = 0.3004225625072866\n",
            "iter = 105;loss = 0.29976913293203944\n",
            "iter = 106;loss = 0.2991293082989314\n",
            "iter = 107;loss = 0.29850264763953605\n",
            "iter = 108;loss = 0.2978887266963093\n",
            "iter = 109;loss = 0.2972871372620717\n",
            "iter = 110;loss = 0.29669748654606165\n",
            "iter = 111;loss = 0.29611939656502223\n",
            "iter = 112;loss = 0.2955525035588769\n",
            "iter = 113;loss = 0.29499645742947594\n",
            "iter = 114;loss = 0.2944509212019599\n",
            "iter = 115;loss = 0.2939155705074621\n",
            "iter = 116;loss = 0.2933900930866455\n",
            "iter = 117;loss = 0.29287418831278594\n",
            "iter = 118;loss = 0.2923675667341053\n",
            "iter = 119;loss = 0.2918699496342316\n",
            "iter = 120;loss = 0.291381068610318\n",
            "iter = 121;loss = 0.2909006651678114\n",
            "iter = 122;loss = 0.29042849033158535\n",
            "iter = 123;loss = 0.2899643042723326\n",
            "iter = 124;loss = 0.2895078759481493\n",
            "iter = 125;loss = 0.28905898276005193\n",
            "iter = 126;loss = 0.2886174102215697\n",
            "iter = 127;loss = 0.2881829516412943\n",
            "iter = 128;loss = 0.2877554078180365\n",
            "iter = 129;loss = 0.2873345867483414\n",
            "iter = 130;loss = 0.2869203033454323\n",
            "iter = 131;loss = 0.28651237916945443\n",
            "iter = 132;loss = 0.28611064216844445\n",
            "iter = 133;loss = 0.2857149264295414\n",
            "iter = 134;loss = 0.2853250719402642\n",
            "iter = 135;loss = 0.2849409243590076\n",
            "iter = 136;loss = 0.28456233479503623\n",
            "iter = 137;loss = 0.28418915959679525\n",
            "iter = 138;loss = 0.28382126014906933\n",
            "iter = 139;loss = 0.28345850267779443\n",
            "iter = 140;loss = 0.28310075806304746\n",
            "iter = 141;loss = 0.28274790165901903\n",
            "iter = 142;loss = 0.2823998131215528\n",
            "iter = 143;loss = 0.2820563762421295\n",
            "iter = 144;loss = 0.28171747878888825\n",
            "iter = 145;loss = 0.2813830123534832\n",
            "iter = 146;loss = 0.28105287220441244\n",
            "iter = 147;loss = 0.2807269571458962\n",
            "iter = 148;loss = 0.28040516938248017\n",
            "iter = 149;loss = 0.2800874143890177\n",
            "iter = 150;loss = 0.27977360078563057\n",
            "iter = 151;loss = 0.27946364021790515\n",
            "iter = 152;loss = 0.27915744724155744\n",
            "iter = 153;loss = 0.2788549392117877\n",
            "iter = 154;loss = 0.2785560361770379\n",
            "iter = 155;loss = 0.2782606607768955\n",
            "iter = 156;loss = 0.2779687381440684\n",
            "iter = 157;loss = 0.2776801958101514\n",
            "iter = 158;loss = 0.27739496361522226\n",
            "iter = 159;loss = 0.27711297362092857\n",
            "iter = 160;loss = 0.27683416002708644\n",
            "iter = 161;loss = 0.27655845909141\n",
            "iter = 162;loss = 0.27628580905261\n",
            "iter = 163;loss = 0.2760161500563613\n",
            "iter = 164;loss = 0.275749424084249\n",
            "iter = 165;loss = 0.2754855748855303\n",
            "iter = 166;loss = 0.27522454791146067\n",
            "iter = 167;loss = 0.27496629025245894\n",
            "iter = 168;loss = 0.27471075057740674\n",
            "iter = 169;loss = 0.27445787907557223\n",
            "iter = 170;loss = 0.274207627400761\n",
            "iter = 171;loss = 0.2739599486175414\n",
            "iter = 172;loss = 0.27371479714980623\n",
            "iter = 173;loss = 0.27347212873107946\n",
            "iter = 174;loss = 0.27323190035696787\n",
            "iter = 175;loss = 0.2729940702393455\n",
            "iter = 176;loss = 0.27275859776248057\n",
            "iter = 177;loss = 0.2725254434406756\n",
            "iter = 178;loss = 0.27229456887773684\n",
            "iter = 179;loss = 0.27206593672783524\n",
            "iter = 180;loss = 0.2718395106581619\n",
            "iter = 181;loss = 0.2716152553127393\n",
            "iter = 182;loss = 0.27139313627790307\n",
            "iter = 183;loss = 0.2711731200487936\n",
            "iter = 184;loss = 0.2709551739976495\n",
            "iter = 185;loss = 0.2707392663427675\n",
            "iter = 186;loss = 0.27052536611898065\n",
            "iter = 187;loss = 0.27031344314942723\n",
            "iter = 188;loss = 0.2701034680179328\n",
            "iter = 189;loss = 0.2698954120428809\n",
            "iter = 190;loss = 0.26968924725191995\n",
            "iter = 191;loss = 0.2694849463576834\n",
            "iter = 192;loss = 0.2692824827343591\n",
            "iter = 193;loss = 0.26908183039534433\n",
            "iter = 194;loss = 0.26888296397150424\n",
            "iter = 195;loss = 0.26868585869051975\n",
            "iter = 196;loss = 0.26849049035680217\n",
            "iter = 197;loss = 0.2682968353323605\n",
            "iter = 198;loss = 0.26810487051822884\n",
            "iter = 199;loss = 0.2679145733367812\n",
            "iter = 200;loss = 0.267725921714557\n",
            "iter = 201;loss = 0.2675388940658203\n",
            "iter = 202;loss = 0.2673534692767643\n",
            "iter = 203;loss = 0.26716962669015665\n",
            "iter = 204;loss = 0.2669873460908994\n",
            "iter = 205;loss = 0.26680660769165926\n",
            "iter = 206;loss = 0.266627392119548\n",
            "iter = 207;loss = 0.2664496804028192\n",
            "iter = 208;loss = 0.26627345395847235\n",
            "iter = 209;loss = 0.2660986945800979\n",
            "iter = 210;loss = 0.2659253844261737\n",
            "iter = 211;loss = 0.2657535060088571\n",
            "iter = 212;loss = 0.26558304218328455\n",
            "iter = 213;loss = 0.26541397613701284\n",
            "iter = 214;loss = 0.26524629138014894\n",
            "iter = 215;loss = 0.2650799717355603\n",
            "iter = 216;loss = 0.26491500132972207\n",
            "iter = 217;loss = 0.26475136458373655\n",
            "iter = 218;loss = 0.2645890462045534\n",
            "iter = 219;loss = 0.264428031176904\n",
            "iter = 220;loss = 0.26426830475514046\n",
            "iter = 221;loss = 0.2641098524555381\n",
            "iter = 222;loss = 0.26395266004895845\n",
            "iter = 223;loss = 0.26379671355354517\n",
            "iter = 224;loss = 0.26364199922791726\n",
            "iter = 225;loss = 0.2634885035644811\n",
            "iter = 226;loss = 0.2633362132829816\n",
            "iter = 227;loss = 0.26318511532438915\n",
            "iter = 228;loss = 0.2630351968448115\n",
            "iter = 229;loss = 0.2628864452098689\n",
            "iter = 230;loss = 0.26273884798903385\n",
            "iter = 231;loss = 0.2625923929502891\n",
            "iter = 232;loss = 0.2624470680550194\n",
            "iter = 233;loss = 0.26230286145294984\n",
            "iter = 234;loss = 0.2621597614773623\n",
            "iter = 235;loss = 0.2620177566404386\n",
            "iter = 236;loss = 0.26187683562877584\n",
            "iter = 237;loss = 0.2617369872989777\n",
            "iter = 238;loss = 0.26159820067365225\n",
            "iter = 239;loss = 0.2614604649372004\n",
            "iter = 240;loss = 0.2613237694319077\n",
            "iter = 241;loss = 0.26118810365428385\n",
            "iter = 242;loss = 0.2610534572513199\n",
            "iter = 243;loss = 0.2609198200170147\n",
            "iter = 244;loss = 0.2607871818889493\n",
            "iter = 245;loss = 0.2606555329449464\n",
            "iter = 246;loss = 0.2605248633999827\n",
            "iter = 247;loss = 0.2603951636030036\n",
            "iter = 248;loss = 0.260266424034046\n",
            "iter = 249;loss = 0.2601386353012342\n",
            "iter = 250;loss = 0.2600117881380668\n",
            "iter = 251;loss = 0.259885873400759\n",
            "iter = 252;loss = 0.25976088206546816\n",
            "iter = 253;loss = 0.25963680522593946\n",
            "iter = 254;loss = 0.25951363409092354\n",
            "iter = 255;loss = 0.2593913599818986\n",
            "iter = 256;loss = 0.2592699743306664\n",
            "iter = 257;loss = 0.2591494686771946\n",
            "iter = 258;loss = 0.25902983466745483\n",
            "iter = 259;loss = 0.25891106405132774\n",
            "iter = 260;loss = 0.2587931486804569\n",
            "iter = 261;loss = 0.25867608050643626\n",
            "iter = 262;loss = 0.2585598515788617\n",
            "iter = 263;loss = 0.2584444540433662\n",
            "iter = 264;loss = 0.2583298801399636\n",
            "iter = 265;loss = 0.25821612220121\n",
            "iter = 266;loss = 0.2581031726505236\n",
            "iter = 267;loss = 0.25799102400058194\n",
            "iter = 268;loss = 0.25787966885161206\n",
            "iter = 269;loss = 0.2577690998899947\n",
            "iter = 270;loss = 0.25765930988659763\n",
            "iter = 271;loss = 0.2575502916953748\n",
            "iter = 272;loss = 0.25744203825199913\n",
            "iter = 273;loss = 0.2573345425723476\n",
            "iter = 274;loss = 0.2572277977511981\n",
            "iter = 275;loss = 0.257121796961\n",
            "iter = 276;loss = 0.25701653345045034\n",
            "iter = 277;loss = 0.25691200054333785\n",
            "iter = 278;loss = 0.25680819163733026\n",
            "iter = 279;loss = 0.25670510020275844\n",
            "iter = 280;loss = 0.2566027197814722\n",
            "iter = 281;loss = 0.2565010439857165\n",
            "iter = 282;loss = 0.2564000664970993\n",
            "iter = 283;loss = 0.2562997810653951\n",
            "iter = 284;loss = 0.2562001815076404\n",
            "iter = 285;loss = 0.2561012617070463\n",
            "iter = 286;loss = 0.2560030156120309\n",
            "iter = 287;loss = 0.25590543723525266\n",
            "iter = 288;loss = 0.25580852065265025\n",
            "iter = 289;loss = 0.2557122600025627\n",
            "iter = 290;loss = 0.25561664948478535\n",
            "iter = 291;loss = 0.2555216833597217\n",
            "iter = 292;loss = 0.255427355947546\n",
            "iter = 293;loss = 0.25533366162731314\n",
            "iter = 294;loss = 0.255240594836129\n",
            "iter = 295;loss = 0.25514815006848557\n",
            "iter = 296;loss = 0.25505632187527166\n",
            "iter = 297;loss = 0.2549651048632134\n",
            "iter = 298;loss = 0.2548744936939654\n",
            "iter = 299;loss = 0.25478448308345325\n",
            "iter = 300;loss = 0.2546950678011367\n",
            "iter = 301;loss = 0.2546062426693355\n",
            "iter = 302;loss = 0.2545180025625199\n",
            "iter = 303;loss = 0.2544303424065961\n",
            "iter = 304;loss = 0.25434325717830786\n",
            "iter = 305;loss = 0.2542567419045989\n",
            "iter = 306;loss = 0.2541707916618993\n",
            "iter = 307;loss = 0.25408540157556536\n",
            "iter = 308;loss = 0.2540005668192664\n",
            "iter = 309;loss = 0.2539162826143875\n",
            "iter = 310;loss = 0.2538325442293963\n",
            "iter = 311;loss = 0.2537493469793045\n",
            "iter = 312;loss = 0.2536666862251591\n",
            "iter = 313;loss = 0.2535845573733572\n",
            "iter = 314;loss = 0.2535029558752066\n",
            "iter = 315;loss = 0.2534218772263944\n",
            "iter = 316;loss = 0.25334131696636136\n",
            "iter = 317;loss = 0.2532612706778818\n",
            "iter = 318;loss = 0.253181733986532\n",
            "iter = 319;loss = 0.25310270256017187\n",
            "iter = 320;loss = 0.25302417210850503\n",
            "iter = 321;loss = 0.2529461383825098\n",
            "iter = 322;loss = 0.2528685971740304\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-e4275ecf8564>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# или comp = LinearReg(eta=1e-1) – запускает полный градиентный спуск с max_iter=10000 и eta=1e-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcomp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearReg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-64aacd1e2304>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, target)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-64aacd1e2304>\u001b[0m in \u001b[0;36mcalc_loss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \"\"\" \n\u001b[1;32m    100\u001b[0m         \u001b[0;31m#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0my_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-64aacd1e2304>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "r-Bi6_AopcuB",
        "outputId": "924a65ea-9768-4fbd-f964-ec7e2fe7a08c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
        "\n",
        "ax.plot(comp.loss_history)\n",
        "ax.set_xlabel('Итерация')\n",
        "ax.set_ylabel('Значение функции потерь')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}