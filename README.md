# Gradient Descent Implementation in Matrix Form

## The Primary Concept
Gradient is a vector of partial derivatives which denotes the direction of greatest change of a scalar function. 

Gradient descent is an algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient of the function at the current point.

<img src="https://saugatbhattarai.com.np/wp-content/uploads/2018/06/gradient-descent-1.jpg" alt="Simplified example of gradient descent" width="460" title="Simplified example of gradient descent">

<sub><sup>WHAT IS GRADIENT DESCENT IN MACHINE LEARNING? / Saugat Bhattarai. – URL: https://saugatbhattarai.com.np/what-is-gradient-descent-in-machine-learning/</sup></sub>
## The Algorithm

<img src="https://github.com/Irinoise/gradient-descent-implementation/blob/main/GD%20algorithm.png" alt="Gradient descent algorithm" width="460" title="Gradient descent algorithm">

## Learning Rate
Сonstant **big learning rate** would lead to "jumping" around the minimum point. 

Constant **small learning rate** would lead to taking a lot of time to converge.

<img src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/gradient-descent-learning-rate.png" alt="Learning rate" width="460" title="Learning rate">

<sub><sup>Gradient Descent in Machine Learning: A Basic Introduction / Niklas Donges. – URL: https://builtin.com/data-science/gradient-descent</sup></sub>
